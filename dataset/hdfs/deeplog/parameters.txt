model_name: deeplog
dataset_name: hdfs
device: cpu
data_dir: ./dataset/hdfs/
output_dir: dataset/hdfs/
run_dir: runs/2023-03-19_hdfs_sample
folder: hdfs/
log_file: sample_log_file.log
sample_size: 10000
sample_log_file: sample_log_file.log
parser_type: drain
log_format: <Date> <Time> <Pid> <Level> <Component> <Content>
regex: []
keep_para: False
st: 0.3
depth: 3
max_child: 100
tau: 0.5
is_process: True
is_instance: False
train_file: train_fixed100_instances.pkl
test_file: test_fixed100_instances.pkl
window_type: session
session_level: None
window_size: 5
step_size: 1
train_size: 0.1
train_ratio: 1.0
valid_ratio: 0.1
test_ratio: 1.0
max_epoch: 10
n_epochs_stop: 10
n_warm_up_epoch: 0
batch_size: 1024
lr: 0.001
is_logkey: True
random_sample: False
is_time: False
min_freq: 1
seq_len: 10
min_len: 10
max_len: 512
mask_ratio: 0.5
adaptive_window: False
deepsvdd_loss: False
deepsvdd_loss_test: False
scale: None
hidden: 256
layers: 4
attn_heads: 4
num_workers: 5
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.0
sample: sliding_window
history_size: 10
embeddings: embeddings.json
sequentials: True
quantitatives: True
semantics: False
parameters: False
input_size: 1
hidden_size: 128
num_layers: 2
embedding_dim: 50
accumulation_step: 5
optimizer: adam
lr_decay_ratio: 0.1
num_candidates: 20
log_freq: 100
resume_path: False
num_encoder_layers: 1
num_decoder_layers: 1
dim_model: 300
num_heads: 8
dim_feedforward: 2048
transformers_dropout: 0.1
model_dir: dataset/hdfs/deeplog/
train_vocab: dataset/hdfs/train.pkl
vocab_path: dataset/hdfs/deeplog_vocab.pkl
model_path: dataset/hdfs/deeplog/deeplog.pth
scale_path: dataset/hdfs/deeplog/scale.pkl
